# compare two excel files

import pandas as pd

def compare_excel_sheets(file1, file2):
    # Read both Excel files
    xls1 = pd.ExcelFile(file1)
    xls2 = pd.ExcelFile(file2)

    # Get sheet names from both files
    sheet_names1 = xls1.sheet_names
    sheet_names2 = xls2.sheet_names

    # Check for new sheets
    new_sheets = set(sheet_names2) - set(sheet_names1)
    if new_sheets:
        print("New sheets added in file2:", new_sheets)

    # Iterate over common sheet names
    common_sheets = set(sheet_names1) & set(sheet_names2)
    for sheet_name in common_sheets:
        df1 = pd.read_excel(file1, sheet_name)
        df2 = pd.read_excel(file2, sheet_name)

        # Check if data underneath the sheet changed
        if not df1.equals(df2):
            print(f"Changes detected in sheet '{sheet_name}':")
            # You can add more detailed comparison logic here if needed

# Provide paths to the Excel files
file_path1 = 'file1.xlsx'
file_path2 = 'file2.xlsx'

# Compare Excel files
compare_excel_sheets(file_path1, file_path2)




# download a file from share point to local

from shareplum import Site
from shareplum import Office365

# SharePoint site URL and credentials
site_url = "https://your_sharepoint_site_url"
username = "your_username"
password = "your_password"

# Path to the file on SharePoint
file_path = "/sites/your_site_name/your_document_library/your_file.xlsx"

# Local path where the downloaded file will be saved
local_file_path = "local_path_to_save_downloaded_file/your_file.xlsx"

# Initialize Office365 session
authcookie = Office365(site_url, username=username, password=password).GetCookies()

# Connect to the SharePoint site
site = Site(site_url, authcookie=authcookie)

# Download the file
site.get_file(file_path, local_file_path)


# rename an old excel file in DBFS

# Specify the old and new file paths
old_file_path = "/path/to/old/file.txt"
new_file_path = "/path/to/new/file.txt"

# Rename the file using dbutils.fs.mv() function
dbutils.fs.mv(old_file_path, new_file_path)







import requests
import json

class Utilities:
    __instance = None

    def __init__(self):
        """
        Private constructor to prevent direct instantiation.
        """
        if Utilities.__instance is not None:
            raise Exception("This class is a singleton! Use get_instance() method instead.")
        else:
            Utilities.__instance = self

    @classmethod
    def get_instance(cls):
        """
        Get instance of Utilities class.

        Returns:
            Utilities: Instance of Utilities class.
        """
        if cls.__instance is None:
            cls.__instance = Utilities()
        return cls.__instance

    @staticmethod
    def send_email_via_logic_app(logic_app_url: str, recipient: str, subject: str, body: str):
        """
        Send email via Azure Logic App.

        Args:
            logic_app_url (str): URL of the Azure Logic App endpoint.
            recipient (str): Email address of the recipient.
            subject (str): Subject of the email.
            body (str): Body content of the email.

        Returns:
            None
        """
        headers = {
            'Content-Type': 'application/json'
        }

        email_data = {
            "recipient": recipient,
            "subject": subject,
            "body": body
        }

        try:
            response = requests.post(logic_app_url, headers=headers, data=json.dumps(email_data))
            if response.status_code == 200:
                print("Email sent successfully!")
            else:
                print("Failed to send email. Status code:", response.status_code)
        except Exception as e:
            print("An error occurred while sending email:", e)

    @staticmethod
    def is_prime(number):
        """
        Check if a number is prime.

        Args:
            number (int): Number to check.

        Returns:
            bool: True if the number is prime, False otherwise.
        """
        if number <= 1:
            return False
        for i in range(2, int(number**0.5) + 1):
            if number % i == 0:
                return False
        return True

    def __private_method(self):
        """
        Example private method.
        """
        print("This is a private method.")

    def public_method(self):
        """
        Example public method.
        """
        print("This is a public method.")

# Example usage:
if __name__ == "__main__":
    # Accessing the instance of the Utilities class
    utilities = Utilities.get_instance()

    # Using the send_email_via_logic_app static method
    logic_app_url = "YOUR_LOGIC_APP_URL_HERE"
    recipient = "recipient@example.com"
    subject = "Test Email"
    body = "This is a test email sent via Azure Logic App."
    utilities.send_email_via_logic_app(logic_app_url, recipient, subject, body)

    # Using public method
    utilities.public_method()

    # Accessing a static method
    print("Is 7 a prime number?", Utilities.is_prime(7))

    # Attempting to access private method (will raise AttributeError)
    try:
        utilities.__private_method()
    except AttributeError:
        print("Cannot access private method directly.")



import logging
from opencensus.ext.azure.log_exporter import AzureLogHandler

def configure_logger():
    logger = logging.getLogger(__name__)
    
    # Remove all existing handlers
    while logger.handlers:
        logger.removeHandler(logger.handlers[0])

    # Add Azure Log Handler only if it does not exist
    if not any(isinstance(handler, AzureLogHandler) for handler in logger.handlers):
        handler = AzureLogHandler(connection_string='InstrumentationKey=your_instrumentation_key')
        logger.addHandler(handler)
    
    # Set logging level
    logger.setLevel(logging.INFO)

    return logger

# Configure logger
logger = configure_logger()

# Log a test message
logger.info("This is a test log message.")







1. Coding Questions: Pyspark:
You've been given some CSV files like karnataka.csv and maharashtra.csv in an ADLS location, each containing columns for first_name, last_name, age, sex, and location.
Your task is to add a new column called state to each DataFrame. The state column should contain the state name extracted from the filename.
For example:
For karnataka.csv, the state column should contain the value 'karnataka'.
For maharashtra.csv, the state column should contain the value 'maharashtra'.
Your solution should utilize PySpark to efficiently handle large-scale data processing tasks.

2. Coding Question: Python/ DSA:
A. Reverse a string without using reverse function
B. Write a python program to implement following. Given a pattern and a string. If string matches the pattern return true else return false. 
Ex: 
Pattern – “abba”, String – “dog cat cat dog” will return true. 
Pattern – “aba”, String – “dog dog cat” will return false.

3. Coding Question: SQL
Find the 3-month rolling average of total revenue from purchases given a table with users, their purchase amount, and date purchased (YYYY-MM-DD). Output the year-month (YYYY-MM) and 3-month rolling average of revenue, sorted from earliest month to latest month.

4. Explain Spark Architecture.

5. What is Z-ordering?

6. Difference between Datamart, Datawarehouse and Deltalake

7. Compare performance of Managed and External Table




- Python + Pandas = Data Manipulation
- Python + Pyspark = Big Data Processing
- Python + Scikit-Learn = Machine Learning
- Python + TensorFlow = Deep Learning
- Python + Matplotlib = Data Visualization
- Python + Seaborn = Advanced Visualization
- Python + Flask = Web Development
- Python + Pygame = Game Development
- Python + Kivy = Mobile App Development
- Python + Django = Full Stack Web Development
- Python + NumPy = Numerical Computation
- Python + Keras = Neural Networks
- Python + Plotly = Interactive Visualization
- Python + FastAPI = Fast Web APIs
- Python + Tkinter = GUI Development
- Python + BeautifulSoup = Web Scraping
- Python + Scrapy = Web Crawling
- Python + OpenCV = Computer Vision
- Python + Pillow = Image Processing
- Python + SQLAlchemy = Database Management
- Python + Requests = HTTP Requests
- Python + PyTorch = Deep Learning
- Python + NLTK = Natural Language Processing
- Python + spaCy = Natural Language Processing
- Python + Bokeh = Interactive Visualization
- Python + Dash = Web-based Data Visualization
- Python + Pydantic = Data Validation
- Python + Celery = Task Queue Management
- Python + Twisted = Network Programming
- Python + Asyncio = Asynchronous Programming


##############################App Insight#################################################################


To achieve your requirements, you can follow these steps:

Send logs from Databricks to Azure Application Insights.
Retrieve logs from Application Insights.
Send the logs via email to users.
Here's an example Python code that covers these steps:

Step 1: Send logs from Databricks to Azure Application Insights
First, you need to install the applicationinsights library on your Databricks cluster:

sh
Copy code
pip install applicationinsights
Now, create a function in Databricks to send logs to Azure Application Insights:

python
Copy code
from applicationinsights import TelemetryClient

def send_log_to_appinsights(instrumentation_key, message, properties=None):
    tc = TelemetryClient(instrumentation_key)
    tc.track_event(message, properties)
    tc.flush()

# Usage example:
instrumentation_key = "YOUR_APPINSIGHTS_INSTRUMENTATION_KEY"
send_log_to_appinsights(instrumentation_key, "This is a test log", {"property1": "value1", "property2": "value2"})
Step 2: Retrieve logs from Application Insights
To retrieve logs from Application Insights, you need to query the logs using the Application Insights REST API or Azure Monitor Query SDK. Here, we'll use the REST API for simplicity:

First, install the required library:

sh
Copy code
pip install azure-identity azure-monitor-query
Now, create a function to retrieve logs:

python
Copy code
import requests
from azure.identity import DefaultAzureCredential
from azure.monitor.query import LogsQueryClient

def retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query):
    credential = DefaultAzureCredential()
    client = LogsQueryClient(credential)

    response = client.query_workspace(
        workspace_id=app_id,
        query=query
    )

    return response.tables

# Usage example:
app_id = "YOUR_APPINSIGHTS_APP_ID"
tenant_id = "YOUR_TENANT_ID"
client_id = "YOUR_CLIENT_ID"
client_secret = "YOUR_CLIENT_SECRET"
query = "requests | take 10"

logs = retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query)
Step 3: Send the logs via email to users
Install the smtplib and email libraries:

sh
Copy code
pip install secure-smtplib
Now, create a function to send the logs via email:

python
Copy code
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

def send_email(smtp_server, smtp_port, smtp_user, smtp_password, from_email, to_emails, subject, body):
    msg = MIMEMultipart()
    msg['From'] = from_email
    msg['To'] = ", ".join(to_emails)
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))

    server = smtplib.SMTP(smtp_server, smtp_port)
    server.starttls()
    server.login(smtp_user, smtp_password)
    server.sendmail(from_email, to_emails, msg.as_string())
    server.quit()

# Usage example:
smtp_server = "smtp.example.com"
smtp_port = 587
smtp_user = "your_smtp_username"
smtp_password = "your_smtp_password"
from_email = "from@example.com"
to_emails = ["to1@example.com", "to2@example.com"]
subject = "App Insights Logs"
body = "Here are the logs: \n" + str(logs)

send_email(smtp_server, smtp_port, smtp_user, smtp_password, from_email, to_emails, subject, body)
Putting it all together
You can create a function that combines all these steps:

python
Copy code
def main():
    # Step 1: Send logs to Application Insights
    instrumentation_key = "YOUR_APPINSIGHTS_INSTRUMENTATION_KEY"
    send_log_to_appinsights(instrumentation_key, "This is a test log", {"property1": "value1", "property2": "value2"})
    
    # Step 2: Retrieve logs from Application Insights
    app_id = "YOUR_APPINSIGHTS_APP_ID"
    tenant_id = "YOUR_TENANT_ID"
    client_id = "YOUR_CLIENT_ID"
    client_secret = "YOUR_CLIENT_SECRET"
    query = "requests | take 10"
    
    logs = retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query)
    
    # Step 3: Send the logs via email to users
    smtp_server = "smtp.example.com"
    smtp_port = 587
    smtp_user = "your_smtp_username"
    smtp_password = "your_smtp_password"
    from_email = "from@example.com"
    to_emails = ["to1@example.com", "to2@example.com"]
    subject = "App Insights Logs"
    body = "Here are the logs: \n" + str(logs)
    
    send_email(smtp_server, smtp_port, smtp_user, smtp_password, from_email, to_emails, subject, body)

if __name__ == "__main__":
    main()
Make sure to replace placeholders with your actual values for instrumentation_key, app_id, tenant_id, client_id, client_secret, smtp_server, smtp_port, smtp_user, smtp_password, from_email, and to_emails.

This script will:

Send a log to Application Insights.
Retrieve the latest logs from Application Insights.
Send the retrieved logs to the specified email addresses.
I don't have smtp username password, suggest using appinsight alerts
ChatGPT
If you don't have SMTP credentials, you can use Azure Application Insights alerts to send logs or notifications via email. Azure Application Insights provides built-in alerting capabilities that allow you to trigger actions based on log data and metrics. You can configure alerts to send emails directly from the Azure portal without needing an SMTP server.

Here’s how to set up and configure Application Insights alerts to send logs via email:

Step 1: Create a Log Alert in Azure Application Insights
Navigate to Application Insights: Go to the Azure portal and navigate to your Application Insights resource.

Create a Log Alert Rule:

Select "Alerts" from the left-hand menu.
Click on "New alert rule."
Define the scope: Select the Application Insights resource you want to monitor.
Add a condition: Click "Add condition," and select "Custom log search."
Define your query: Enter a Kusto Query Language (KQL) query to define the logs you want to alert on. For example, to alert on failed requests:
kql
Copy code
requests
| where resultCode != "200"
Set the alert logic: Configure the condition to trigger when the number of results meets your threshold.
Configure the Action Group:

Click "Add action group."
Name the action group and provide a short name.
Select "Action type" as "Email/SMS/Push/Voice."
Configure email settings: Enter the email addresses you want to receive the alerts.
Save the action group.
Create and Enable the Alert Rule:

Give the alert rule a name and description.
Set the severity level.
Review and create the alert rule.
Step 2: Retrieve Logs from Application Insights via API (Optional)
If you need to retrieve logs programmatically and send them via another method, you can use the Application Insights REST API or Azure Monitor Query SDK. Here's an example using the REST API:

python
Copy code
import requests
from azure.identity import DefaultAzureCredential
from azure.monitor.query import LogsQueryClient

def retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query):
    credential = DefaultAzureCredential()
    client = LogsQueryClient(credential)

    response = client.query_workspace(
        workspace_id=app_id,
        query=query
    )

    return response.tables

# Usage example:
app_id = "YOUR_APPINSIGHTS_APP_ID"
tenant_id = "YOUR_TENANT_ID"
client_id = "YOUR_CLIENT_ID"
client_secret = "YOUR_CLIENT_SECRET"
query = "requests | where resultCode != '200' | take 10"

logs = retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query)
print(logs)
Step 3: Using Logic Apps for Additional Email Customization (Optional)
If you need more customization for your email notifications, you can use Azure Logic Apps to send emails based on Application Insights alerts.

Create a Logic App: In the Azure portal, create a new Logic App.
Define the Trigger: Use "When a HTTP request is received" as the trigger.
Add Actions:
Parse the JSON received from the alert.
Use the "Send an email" action to configure and send the email.
Integrate with Application Insights: In the alert rule configuration, use the Logic App as the action group.
By using Azure Application Insights alerts and potentially integrating with Logic Apps, you can effectively monitor and send logs via email without needing SMTP credentials.


To retrieve Application Insights logs using Python, you can use the azure-monitor-query package. This package allows you to query Application Insights logs with a specific instrumentation key. Below is a step-by-step guide on how to set this up:

Step 1: Install the necessary package
First, install the azure-monitor-query package if you haven't already:

bash
Copy code
pip install azure-monitor-query
Step 2: Set up the Azure credentials
You'll need Azure credentials to authenticate your queries. You can use Azure Service Principal, Azure Managed Identity, or Azure CLI login.

Here's an example using the DefaultAzureCredential:

python
Copy code
from azure.identity import DefaultAzureCredential
from azure.monitor.query import LogsQueryClient

# Create a LogsQueryClient with DefaultAzureCredential
credential = DefaultAzureCredential()
client = LogsQueryClient(credential)
Step 3: Define your query and execute it
You can define your Kusto Query Language (KQL) query and execute it to retrieve the logs. Here’s an example:

python
Copy code
from datetime import datetime, timedelta

# Define the Application Insights query
query = """
traces
| where timestamp > ago(1h)
| project timestamp, message, severityLevel
| order by timestamp desc
"""

# Replace 'your-instrumentation-key' with your actual instrumentation key
application_id = 'your-instrumentation-key'

# Execute the query
response = client.query_workspace(
    workspace_id=application_id,
    query=query,
    timespan=(datetime.now() - timedelta(hours=1), datetime.now())
)

# Print the results
if response.status == 'Success':
    for table in response.tables:
        for row in table.rows:
            print(row)
else:
    print(f"Query failed with status: {response.status}")
Step 4: Handling authentication
Make sure to handle authentication properly. If you’re using Azure Service Principal, you can use the following code to authenticate:

python
Copy code
from azure.identity import ClientSecretCredential

tenant_id = 'your-tenant-id'
client_id = 'your-client-id'
client_secret = 'your-client-secret'

credential = ClientSecretCredential(tenant_id, client_id, client_secret)
client = LogsQueryClient(credential)
Full Example
python
Copy code
from azure.identity import DefaultAzureCredential
from azure.monitor.query import LogsQueryClient
from datetime import datetime, timedelta

# Set up the credential and client
credential = DefaultAzureCredential()
client = LogsQueryClient(credential)

# Define the Application Insights query
query = """
traces
| where timestamp > ago(1h)
| project timestamp, message, severityLevel
| order by timestamp desc
"""

# Replace 'your-instrumentation-key' with your actual instrumentation key
application_id = 'your-instrumentation-key'

# Execute the query
response = client.query_workspace(
    workspace_id=application_id,
    query=query,
    timespan=(datetime.now() - timedelta(hours=1), datetime.now())
)

# Print the results
if response.status == 'Success':
    for table in response.tables:
        for row in table.rows:
            print(row)
else:
    print(f"Query failed with status: {response.status}")
Replace 'your-instrumentation-key', 'your-tenant-id', 'your-client-id', and 'your-client-secret' with your actual values. This script will retrieve the logs from the last hour and print them.

Make sure to manage your credentials securely and avoid hardcoding them in your script. Use environment variables or Azure Key Vault for sensitive information.







from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkSession
spark = SparkSession.builder.appName("SampleDataFrame").getOrCreate()

# Sample DataFrame creation for demonstration
data = [
    ("California", 1), ("California", 2), ("California", 3),
    ("New York", 4), ("New York", 5), ("New York", 6),
    ("Texas", 7), ("Texas", 8), ("Texas", 9),
    # Add more records as needed
]
columns = ["state", "value"]
df = spark.createDataFrame(data, columns)

# Display the original DataFrame
df.show()

# Take 10% of records per state
fractions = df.select("state").distinct().withColumn("fraction", lit(0.1)).rdd.collectAsMap()
sampled_df = df.sampleBy("state", fractions, seed=42)

# Display the sampled DataFrame
sampled_df.show()

# Stop the SparkSession
spark.stop()



## write output to a file in databricks


import json
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("AppendJSON").getOrCreate()

def append_json_to_file(json_content, file_path="/dbfs/tmp/json_data.txt"):
    """
    Appends JSON content to a file in DBFS.
    
    :param json_content: The JSON content to append (dictionary).
    :param file_path: The path to the file in DBFS.
    """
    try:
        # Read existing content if file exists
        try:
            with open(file_path, "r") as file:
                data = file.read()
                if data:
                    existing_data = json.loads(data)
                else:
                    existing_data = []
        except FileNotFoundError:
            existing_data = []
        
        # Append new JSON content
        existing_data.append(json_content)
        
        # Write updated content back to file
        with open(file_path, "w") as file:
            file.write(json.dumps(existing_data, indent=4))
        
        print("JSON content appended successfully.")
    
    except Exception as e:
        print(f"An error occurred: {e}")

# Example JSON content
json_content_example = {
    "name": "example",
    "value": 42
}

# Append example JSON content to file
append_json_to_file(json_content_example)



import json
import os
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("AppendJSON").getOrCreate()

def create_or_clear_file(file_path="/dbfs/tmp/json_data.txt"):
    """
    Creates a file or clears its content if it already exists.
    
    :param file_path: The path to the file in DBFS.
    """
    with open(file_path, "w") as file:
        file.write("")

def append_json_to_file(json_content, file_path="/dbfs/tmp/json_data.txt"):
    """
    Appends JSON content to a file in DBFS.
    
    :param json_content: The JSON content to append (dictionary).
    :param file_path: The path to the file in DBFS.
    """
    try:
        # Read existing content if file exists and is not empty
        try:
            with open(file_path, "r") as file:
                data = file.read()
                if data.strip():  # Check if file is not empty
                    existing_data = json.loads(data)
                else:
                    existing_data = []
        except FileNotFoundError:
            existing_data = []
        
        # Append new JSON content
        existing_data.append(json_content)
        
        # Write updated content back to file
        with open(file_path, "w") as file:
            file.write(json.dumps(existing_data, indent=4))
        
        print("JSON content appended successfully.")
    
    except Exception as e:
        print(f"An error occurred: {e}")

# Create or clear the file first
create_or_clear_file()

# Example JSON content
json_content_example = {
    "name": "example",
    "value": 42
}

# Append example JSON content to file
append_json_to_file(json_content_example)




Build a databricks widget to compare 2 files.

a> It should take the file input type as excel, csv and JSON and compare the files, after converting both files csv.
b> It should then print rows in first file , not in second file , using exceptAll
c> Then print rows in second file not in first file, using exceptAll
d> Now within the rertreived differences in point b and c , find what column values were actually different and what values were same, using address and building_id as the key


# Import necessary libraries
import pandas as pd
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("FileComparison").getOrCreate()

# Create widgets for file input
dbutils.widgets.text("file_path_1", "")
dbutils.widgets.text("file_path_2", "")
dbutils.widgets.dropdown("file_type_1", "csv", ["csv", "json", "excel"])
dbutils.widgets.dropdown("file_type_2", "csv", ["csv", "json", "excel"])

# Read widget values
file_path_1 = dbutils.widgets.get("file_path_1")
file_path_2 = dbutils.widgets.get("file_path_2")
file_type_1 = dbutils.widgets.get("file_type_1")
file_type_2 = dbutils.widgets.get("file_type_2")

# Function to read different file types and convert to Spark DataFrame
def read_file(file_path, file_type):
    if file_type == "csv":
        return spark.read.option("header", "true").csv(file_path)
    elif file_type == "json":
        return spark.read.json(file_path)
    elif file_type == "excel":
        pdf = pd.read_excel(file_path)
        return spark.createDataFrame(pdf)

# Read both files
df1 = read_file(file_path_1, file_type_1)
df2 = read_file(file_path_2, file_type_2)

# Convert DataFrames to have the same schema by selecting common columns
common_cols = list(set(df1.columns).intersection(set(df2.columns)))
df1 = df1.select(common_cols)
df2 = df2.select(common_cols)

# Print rows in first file not in second file
print("Rows in first file not in second file:")
df1_diff = df1.exceptAll(df2)
df1_diff.show()

# Print rows in second file not in first file
print("Rows in second file not in first file:")
df2_diff = df2.exceptAll(df1)
df2_diff.show()

# Find column differences within the retrieved differences
key_columns = ["address", "building_id"]

# Create a function to find differences in columns
def find_differences(df1, df2, key_cols):
    df1_keyed = df1.select(key_cols + [col for col in df1.columns if col not in key_cols])
    df2_keyed = df2.select(key_cols + [col for col in df2.columns if col not in key_cols])
    df1_keyed = df1_keyed.withColumnRenamed("address", "address_1").withColumnRenamed("building_id", "building_id_1")
    df2_keyed = df2_keyed.withColumnRenamed("address", "address_2").withColumnRenamed("building_id", "building_id_2")
    diff_df = df1_keyed.join(df2_keyed, (df1_keyed.address_1 == df2_keyed.address_2) & (df1_keyed.building_id_1 == df2_keyed.building_id_2), "outer")
    diff_df = diff_df.withColumn("Same/Changed", col("address_1") == col("address_2") & col("building_id_1") == col("building_id_2"))
    return diff_df

# Apply the function to the differences
diff_df = find_differences(df1_diff, df2_diff, key_columns)
diff_df.show()





# Find column differences within the retrieved differences
key_columns = ["address", "building_id"]

# Create a function to find differences in columns
def find_differences(df1, df2, key_cols):
    diff_cols = [col for col in df1.columns if col not in key_cols]
    df1_keyed = df1.select(key_cols + diff_cols)
    df2_keyed = df2.select(key_cols + diff_cols)
    join_expr = [df1_keyed[k] == df2_keyed[k] for k in key_cols]
    
    diff_df = df1_keyed.alias("df1").join(df2_keyed.alias("df2"), join_expr, "fullouter")
    
    for column in diff_cols:
        diff_df = diff_df.withColumn(
            f"{column}_diff",
            when(
                (col(f"df1.{column}").isNull() & col(f"df2.{column}").isNotNull()) | 
                (col(f"df1.{column}").isNotNull() & col(f"df2.{column}").isNull()) | 
                (col(f"df1.{column}") != col(f"df2.{column}")),
                lit("Different")
            ).otherwise(lit("Same"))
        )
    
    return diff_df

# Apply the function to the differences
diff_df = find_differences(df1_diff, df2_diff, key_columns)
diff_df.show(truncate=False)

### print column names

same_columns_expr = concat_ws(", ", *[when(col(f"{col}_diff") == "Same", lit(col)).otherwise(lit(None)) for col in diff_cols])
    different_columns_expr = concat_ws(", ", *[when(col(f"{col}_diff") == "Different", lit(col)).otherwise(lit(None)) for col in diff_cols])
    
    diff_df = diff_df.withColumn("matched", same_columns_expr)
    diff_df = diff_df.withColumn("unmatched", different_columns_expr)
    
    return diff_df

# Apply the function to the differences
diff_df = find_differences(df1_diff, df2_diff, key_columns)
diff_df.select(key_columns + ["matched", "unmatched"]).show(truncate=False)


######################################




# Import necessary libraries
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when, expr, concat_ws

# Initialize Spark session
spark = SparkSession.builder.appName("FileComparison").getOrCreate()

# Create widgets for file input
dbutils.widgets.text("file_path_1", "")
dbutils.widgets.text("file_path_2", "")
dbutils.widgets.dropdown("file_type_1", "csv", ["csv", "json", "excel"])
dbutils.widgets.dropdown("file_type_2", "csv", ["csv", "json", "excel"])

# Read widget values
file_path_1 = dbutils.widgets.get("file_path_1")
file_path_2 = dbutils.widgets.get("file_path_2")
file_type_1 = dbutils.widgets.get("file_type_1")
file_type_2 = dbutils.widgets.get("file_type_2")

# Function to read different file types and convert to Spark DataFrame
def read_file(file_path, file_type):
    if file_type == "csv":
        return spark.read.option("header", "true").csv(file_path)
    elif file_type == "json":
        return spark.read.json(file_path)
    elif file_type == "excel":
        pdf = pd.read_excel(file_path)
        return spark.createDataFrame(pdf)

# Read both files
df1 = read_file(file_path_1, file_type_1)
df2 = read_file(file_path_2, file_type_2)

# Convert DataFrames to have the same schema by selecting common columns
common_cols = list(set(df1.columns).intersection(set(df2.columns)))
df1 = df1.select(common_cols)
df2 = df2.select(common_cols)

# Register DataFrames as temporary views for SQL queries
df1.createOrReplaceTempView("df1")
df2.createOrReplaceTempView("df2")

# Full outer join on address and building_id
joined_df = spark.sql("""
SELECT
  COALESCE(df1.address, df2.address) AS address,
  COALESCE(df1.building_id, df2.building_id) AS building_id,
  df1.*,
  df2.*
FROM df1
FULL OUTER JOIN df2
ON df1.address = df2.address AND df1.building_id = df2.building_id
""")

# Function to determine the status of each row
def determine_status(row):
    if row[2] is None:
        return "row in second not in first"
    elif row[3] is None:
        return "row in first not in second"
    else:
        return "rows which match"

# Function to find differences in columns
def find_column_differences(row):
    diff_columns = []
    for col_name in common_cols:
        if row[f"df1.{col_name}"] != row[f"df2.{col_name}"]:
            diff_columns.append(f"{col_name}: {row[f'df1.{col_name}']} != {row[f'df2.{col_name}']}")
    return ", ".join(diff_columns)

# Apply the function to determine the status and find column differences
joined_df = joined_df.withColumn("status", expr("CASE WHEN df1.address IS NULL THEN 'row in second not in first' WHEN df2.address IS NULL THEN 'row in first not in second' ELSE 'rows which match' END"))

# Filter matched rows and find column differences
matched_df = joined_df.filter("status = 'rows which match'")
diff_df = matched_df.withColumn("column_differences", expr("CASE WHEN " + " OR ".join([f"df1.{col} != df2.{col}" for col in common_cols]) + " THEN 'rows that match but column values are different' ELSE NULL END"))
diff_df = diff_df.filter("column_differences IS NOT NULL")

# Print the results
print("Rows in first not in second:")
joined_df.filter("status = 'row in first not in second'").show(truncate=False)

print("Rows in second not in first:")
joined_df.filter("status = 'row in second not in first'").show(truncate=False)

print("Rows which match:")
matched_df.show(truncate=False)

print("Rows that match but column values are different:")
diff_df.show(truncate=False)

# Create a message for rows that match but have different column values
diff_message_df = diff_df.withColumn("diff_message", concat_ws(", ", *[when(col(f"df1.{col}") != col(f"df2.{col}"), lit(f"{col}: df1={col(f'df1.{col}')} != df2={col(f'df2.{col}')}")).otherwise(lit(None)) for col in common_cols]))
diff_message_df.select("address", "building_id", "diff_message").show(truncate=False)


from pyspark.sql import SparkSession

# Get the SparkSession
spark = SparkSession.builder.getOrCreate()

# Get the Hadoop Configuration
hadoop_conf = spark._jsc.hadoopConfiguration()

# Get the FileSystem
fs = org.apache.hadoop.fs.FileSystem.get(hadoop_conf)

# Get the storage statistics
stats = fs.getStats()
print(f"Capacity: {stats.getCapacity()}")
print(f"Used: {stats.getUsed()}")
print(f"Remaining: {stats.getRemaining()}")

# Convert bytes to a more readable format (e.g., GB)
used_gb = stats.getUsed() / (1024 * 1024 * 1024)
capacity_gb = stats.getCapacity() / (1024 * 1024 * 1024)

print(f"Total storage used: {used_gb:.2f} GB")
print(f"Total storage capacity: {capacity_gb:.2f} GB")




import requests

# URL of the .sql file
file_url = 'https://example.com/path/to/your/largefile.sql'

# DBFS path where the file will be stored
dbfs_path = '/dbfs/tmp/largefile.sql'

# Function to download the file in chunks
def download_large_file(url, dest_path, chunk_size=1024*1024):
    response = requests.get(url, stream=True)
    response.raise_for_status()
    with open(dest_path, 'wb') as file:
        for chunk in response.iter_content(chunk_size=chunk_size):
            if chunk:
                file.write(chunk)
                file.flush()
    print(f"Downloaded file to {dest_path}")

# Temporary local path to save the chunks
local_temp_path = '/tmp/largefile.sql'

# Download the file
download_large_file(file_url, local_temp_path)

# Move the file to DBFS
dbutils.fs.mv(f"file://{local_temp_path}", dbfs_path)

print(f"File moved to DBFS at {dbfs_path}")

#############################################


# Mount the file system if needed
dbutils.fs.mount("source", "/mnt/mountname", "extraConfigs")

# Path to the .sql file
file_path = "/dbfs/mnt/mountname/your_sql_dump_file.sql"

# Read the SQL file
with open(file_path, 'r') as file:
    sql_commands = file.read()

# Split the SQL commands based on ';'
sql_commands = sql_commands.split(';')

# Remove any empty statements
sql_commands = [cmd.strip() for cmd in sql_commands if cmd.strip()]

# Execute SQL commands
for command in sql_commands:
    try:
        spark.sql(command)
    except Exception as e:
        print(f"An error occurred while executing the command: {command}\nError: {str(e)}")

# Load data into DataFrame
df = spark.sql("SELECT * FROM your_table_name")

# Display the DataFrame
df.show()

# Save the DataFrame to a Databricks table
df.write.saveAsTable("your_table_name")

# Or save it as a Delta table
df.write.format("delta").saveAsTable("your_delta_table_name")


###########################


import csv
import sys
from signal import signal, SIGPIPE, SIG_DFL
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Prevents prematurely closed pipes from raising an exception in Python
signal(SIGPIPE, SIG_DFL)

# Allow large content in the dump
csv.field_size_limit(sys.maxsize)

# Initialize Spark session
spark = SparkSession.builder.appName("SQLDumpToCSV").getOrCreate()

def is_insert(line):
    """
    Returns true if the line begins a SQL insert statement.
    """
    return line.startswith('INSERT INTO') or False

def get_values(line):
    """
    Returns the portion of an INSERT statement containing values
    """
    return line.partition('` VALUES ')[2]

def values_sanity_check(values):
    """
    Ensures that values from the INSERT statement meet basic checks.
    """
    assert values
    assert values[0] == '('
    # Assertions have not been raised
    return True

def parse_values(values):
    """
    Given the raw values from a MySQL INSERT statement, convert to list of rows
    """
    latest_row = []
    rows = []

    reader = csv.reader([values], delimiter=',',
                        doublequote=False,
                        escapechar='\\',
                        quotechar="'",
                        strict=True)

    for reader_row in reader:
        for column in reader_row:
            if len(column) == 0 or column == 'NULL':
                latest_row.append(None)
                continue
            if column[0] == "(":
                new_row = False
                if len(latest_row) > 0 and latest_row[-1][-1] == ")":
                    latest_row[-1] = latest_row[-1][:-1]
                    new_row = True
                if new_row:
                    rows.append(latest_row)
                    latest_row = []
                if len(latest_row) == 0:
                    column = column[1:]
            latest_row.append(column)
        if latest_row[-1][-2:] == ");":
            latest_row[-1] = latest_row[-1][:-2]
            rows.append(latest_row)

    return rows

def process_sql_file(file_path, output_path):
    """
    Process the SQL dump file and convert to CSV
    """
    with open(file_path, 'r') as file:
        lines = file.readlines()

    data = []
    for line in lines:
        if is_insert(line):
            values = get_values(line)
            if values_sanity_check(values):
                rows = parse_values(values)
                data.extend(rows)
    
    # Convert to Spark DataFrame
    columns = [f"col_{i}" for i in range(len(data[0]))]
    df = spark.createDataFrame(data, columns)
    
    # Save DataFrame as CSV
    df.write.csv(output_path, header=True, mode='overwrite')

def convert_sql_to_csv(file_path, output_path):
    """
    Main function to call for converting SQL to CSV
    """
    process_sql_file(file_path, output_path)
    print(f"Data successfully converted and saved to {output_path}")

# Example usage
file_path = "/dbfs/mnt/mountname/your_sql_dump_file.sql"  # Path to your SQL dump file
output_path = "/dbfs/mnt/mountname/output_csv_folder"    # Path to save the output CSV

convert_sql_to_csv(file_path, output_path)

#######################################################################################


import sqlparse
import pandas as pd
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("SQLDumpToCSV").getOrCreate()

# Define the path to the SQL dump file
sql_dump_path = "/mnt/<your-mount-name>/path/to/your-dump.sql"

# Function to process and save each table
def process_table(statements, table_name):
    table_data = []
    for statement in statements:
        if statement.get_type() == 'INSERT':
            values = statement.tokens[-1].value
            rows = sqlparse.sql.TokenList(sqlparse.parse(values)).tokens[1].tokens[1:]
            for row in rows:
                table_data.append(row.value.strip().strip('()').split(','))
    
    # Convert to DataFrame
    df = pd.DataFrame(table_data)
    
    # Save DataFrame to CSV
    df.to_csv(f"/dbfs/mnt/<your-mount-name>/{table_name}.csv", index=False)

# Read the SQL dump file in chunks
with open(sql_dump_path, 'r') as file:
    chunk_size = 1024 * 1024 * 10  # 10 MB
    sql_buffer = ""
    table_name = ""
    while chunk := file.read(chunk_size):
        sql_buffer += chunk
        statements = sqlparse.split(sql_buffer)
        for statement in statements[:-1]:
            parsed = sqlparse.parse(statement)[0]
            if parsed.get_type() == 'CREATE':
                table_name = parsed.get_name()
            process_table(parsed, table_name)
        sql_buffer = statements[-1]

# Process the remaining buffer
if sql_buffer:
    parsed = sqlparse.parse(sql_buffer)[0]
    process_table(parsed, table_name)





import pandas as pd

# Load the Excel file with all sheets
file_path = 'your_excel_file.xlsx'
xls = pd.read_excel(file_path, sheet_name=None)

# Iterate through each sheet in the dictionary
for sheet_name, df in xls.items():
    # Trim leading and trailing whitespaces from string columns
    xls[sheet_name] = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Save the cleaned data back to an Excel file
cleaned_file_path = 'cleaned_excel_file.xlsx'
with pd.ExcelWriter(cleaned_file_path) as writer:
    for sheet_name, df in xls.items():
        df.to_excel(writer, sheet_name=sheet_name, index=False)

print(f"Whitespace cleaned and saved to {cleaned_file_path}")




import requests
from pyspark.sql import SparkSession
import pandas as pd

# Initialize Spark session
spark = SparkSession.builder.appName("AppInsightsToDatabricks").getOrCreate()

app_id = "<YOUR_APP_ID>"
api_key = "<YOUR_API_KEY>"
query = "requests | limit 10"

url = f"https://api.applicationinsights.io/v1/apps/{app_id}/query"
headers = {"x-api-key": api_key}
params = {"query": query}

response = requests.get(url, headers=headers, params=params)
data = response.json()["tables"][0]["rows"]

# Convert data to pandas DataFrame and then to Spark DataFrame
df = pd.DataFrame(data)
spark_df = spark.createDataFrame(df)

# Show the Spark DataFrame
spark_df.show()


import requests

# Replace with your Application ID and API key
app_id = "<YOUR_APP_ID>"
api_key = "<YOUR_API_KEY>"

# Your Kusto query
query = "requests | limit 10"

# Construct the URL and headers
url = f"https://api.applicationinsights.io/v1/apps/{app_id}/query"
headers = {
    "x-api-key": api_key
}
params = {
    "query": query
}

# Make the API request
response = requests.get(url, headers=headers, params=params)
data = response.json()

# Retrieve column names
columns = [col["name"] for col in data["tables"][0]["columns"]]
print("Column Names:", columns)

# Retrieve the rows of data
rows = data["tables"][0]["rows"]

# Combine column names and rows into a structured format
formatted_data = [dict(zip(columns, row)) for row in rows]

# Print the structured data
for item in formatted_data:
    print(item)




import requests

app_id = "<YOUR_APP_ID>"
api_key = "<YOUR_API_KEY>"
query = "requests | where timestamp >= ago(7d)"

url = f"https://api.applicationinsights.io/v1/apps/{app_id}/query"
headers = {
    "x-api-key": api_key
}
params = {
    "query": query
}

all_data = []

while True:
    response = requests.get(url, headers=headers, params=params)
    data = response.json()

    # Retrieve and store the data
    columns = [col["name"] for col in data["tables"][0]["columns"]]
    rows = data["tables"][0]["rows"]
    all_data.extend([dict(zip(columns, row)) for row in rows])

    # Check for continuation token
    if "continuationToken" in data:
        params["continuationToken"] = data["continuationToken"]
    else:
        break

print(f"Retrieved {len(all_data)} records.")

##########################


import requests
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType

# Initialize SparkSession
spark = SparkSession.builder.appName("AppInsightsData").getOrCreate()

# Application Insights details
app_id = "<YOUR_APP_ID>"
api_key = "<YOUR_API_KEY>"
url = f"https://api.applicationinsights.io/v1/apps/{app_id}/query"
headers = {
    "x-api-key": api_key
}

def fetch_data_for_range(start_date, end_date):
    query = f"requests | where timestamp >= datetime({start_date}) and timestamp < datetime({end_date})"
    params = {"query": query}
    all_rows = []

    while True:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()

        # Retrieve column names and rows
        columns = [col["name"] for col in data["tables"][0]["columns"]]
        rows = data["tables"][0]["rows"]
        all_rows.extend([dict(zip(columns, row)) for row in rows])

        # Check for continuation token if applicable
        if "continuationToken" in data:
            params["continuationToken"] = data["continuationToken"]
        else:
            break

    return all_rows, columns

# Define the date range (1 year broken into smaller intervals)
start_date = datetime.now() - timedelta(days=365)
chunk_size = 30  # 30 days intervals
data_chunks = []
columns = None

while True:
    interval_start = start_date
    interval_end = start_date + timedelta(days=chunk_size)
    if interval_end > datetime.now():
        interval_end = datetime.now()

    chunk_data, chunk_columns = fetch_data_for_range(interval_start.isoformat(), interval_end.isoformat())
    if not chunk_data:
        break

    if columns is None:
        columns = chunk_columns

    data_chunks.extend(chunk_data)
    start_date = interval_end

    if interval_end >= datetime.now():
        break

# Define schema based on columns
schema = StructType([StructField(col, StringType(), True) for col in columns])

# Convert data to Spark DataFrame
df = spark.createDataFrame(data_chunks, schema=schema)

# Show DataFrame
df.show()

###############
###############


import requests
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType

# Initialize SparkSession
spark = SparkSession.builder.appName("AppInsightsData").getOrCreate()

# Replace with your Application ID and API key
app_id = "<YOUR_APP_ID>"
api_key = "<YOUR_API_KEY>"

# Construct the URL and headers
url = f"https://api.applicationinsights.io/v1/apps/{app_id}/query"
headers = {
    "x-api-key": api_key
}

def fetch_data_for_range(start_date, end_date, continuation_token=None):
    query = f"requests | where timestamp >= datetime({start_date}) and timestamp < datetime({end_date})"
    params = {"query": query}
    if continuation_token:
        params["continuationToken"] = continuation_token
    all_rows = []

    while True:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()

        # Retrieve column names and rows
        columns = [col["name"] for col in data["tables"][0]["columns"]]
        rows = data["tables"][0]["rows"]
        all_rows.extend([dict(zip(columns, row)) for row in rows])

        # Check for continuation token if applicable
        if "continuationToken" in data:
            continuation_token = data["continuationToken"]
            params["continuationToken"] = continuation_token
        else:
            break

    return all_rows, columns

# Define the date range (1 year broken into smaller intervals)
start_date = datetime.now() - timedelta(days=365)
chunk_size = 30  # 30 days intervals
data_chunks = []
columns = None

while True:
    interval_start = start_date
    interval_end = start_date + timedelta(days=chunk_size)
    if interval_end > datetime.now():
        interval_end = datetime.now()

    chunk_data, chunk_columns = fetch_data_for_range(interval_start.isoformat(), interval_end.isoformat())
    if not chunk_data:
        break

    if columns is None:
        columns = chunk_columns

    data_chunks.extend(chunk_data)
    start_date = interval_end

    if interval_end >= datetime.now():
        break

# Define schema based on columns
schema = StructType([StructField(col, StringType(), True) for col in columns])

# Convert data to Spark DataFrame
df = spark.createDataFrame(data_chunks, schema=schema)

# Show DataFrame
df.show()

# Optionally, save the DataFrame to a file or table for further analysis
df.write.format("delta").save("/mnt/app_insights_data")

##############


import requests
import pandas as pd
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder.appName("AppInsightsData").getOrCreate()

# Replace with your Application ID and API key
app_id = "<YOUR_APP_ID>"
api_key = "<YOUR_API_KEY>"

# Define the base URL and headers for API requests
url = f"https://api.applicationinsights.io/v1/apps/{app_id}/query"
headers = {
    "x-api-key": api_key
}

def fetch_data(query, start_date, end_date, continuation_token=None):
    params = {
        "query": query.format(start_date=start_date, end_date=end_date)
    }
    if continuation_token:
        params["continuationToken"] = continuation_token

    response = requests.get(url, headers=headers, params=params)
    response.raise_for_status()  # Ensure we catch any HTTP errors
    data = response.json()

    return data

# Define your Kusto query with placeholders for dates
query = "requests | where timestamp >= datetime({start_date}) and timestamp < datetime({end_date})"

# Define the date range (e.g., 1 year in 30-day chunks)
start_date = datetime.now() - timedelta(days=365)
chunk_size = 30  # Days
all_data = []
columns = None

while start_date < datetime.now():
    end_date = start_date + timedelta(days=chunk_size)
    if end_date > datetime.now():
        end_date = datetime.now()

    # Fetch data for the current chunk
    data = fetch_data(query, start_date.isoformat(), end_date.isoformat())
    
    # Process columns and rows
    if columns is None:
        columns = [col["name"] for col in data["tables"][0]["columns"]]
    
    rows = data["tables"][0]["rows"]
    all_data.extend(rows)
    
    # Handle continuation token
    if "continuationToken" in data:
        continuation_token = data["continuationToken"]
    else:
        continuation_token = None

    # Move to the next chunk
    start_date = end_date

# Create a Pandas DataFrame from the collected data
df_pandas = pd.DataFrame(all_data, columns=columns)

# Convert the Pandas DataFrame to a Spark DataFrame
df_spark = spark.createDataFrame(df_pandas)

# Show the Spark DataFrame
df_spark.show()

# Optionally, save the DataFrame to a file or table for further analysis
df_spark.write.format("delta").save("/mnt/app_insights_data")


##############


import requests
import pandas as pd
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder.appName("AppInsightsData").getOrCreate()

# Replace with your Application ID and API key
app_id = "<YOUR_APP_ID>"
api_key = "<YOUR_API_KEY>"

# Define the base URL and headers for API requests
url = f"https://api.applicationinsights.io/v1/apps/{app_id}/query"
headers = {
    "x-api-key": api_key
}

def fetch_data(query, start_date, end_date, continuation_token=None):
    params = {
        "query": query.format(start_date=start_date, end_date=end_date)
    }
    if continuation_token:
        params["continuationToken"] = continuation_token

    response = requests.get(url, headers=headers, params=params)
    response.raise_for_status()  # Ensure we catch any HTTP errors
    data = response.json()

    return data

def get_date_ranges(period):
    now = datetime.now()
    if period == "today":
        return [(now.date().isoformat(), now.date().isoformat())]
    elif period == "last_1_day":
        start_date = now - timedelta(days=1)
        return [(start_date.date().isoformat(), now.date().isoformat())]
    elif period == "last_2_days":
        start_date = now - timedelta(days=2)
        return [(start_date.date().isoformat(), now.date().isoformat())]
    elif period == "past_year":
        start_date = now - timedelta(days=365)
        return [(start_date.date().isoformat(), now.date().isoformat())]
    else:
        raise ValueError("Unsupported period")

def process_data_for_range(start_date, end_date):
    query = "requests | where timestamp >= datetime({start_date}) and timestamp < datetime({end_date})"
    all_data = []
    columns = None
    continuation_token = None

    while True:
        data = fetch_data(query, start_date, end_date, continuation_token)
        
        if columns is None:
            columns = [col["name"] for col in data["tables"][0]["columns"]]
        
        rows = data["tables"][0]["rows"]
        all_data.extend(rows)
        
        if "continuationToken" in data:
            continuation_token = data["continuationToken"]
        else:
            break

    return all_data, columns

# Define the period to fetch data for
period = "past_year"  # Options: "today", "last_1_day", "last_2_days", "past_year"

date_ranges = get_date_ranges(period)
data_chunks = []

for start_date, end_date in date_ranges:
    # Fetch data in chunks and append to Spark DataFrame
    chunk_data, columns = process_data_for_range(start_date, end_date)
    
    # Create a Pandas DataFrame for the current chunk
    df_pandas = pd.DataFrame(chunk_data, columns=columns)
    
    # Convert Pandas DataFrame to Spark DataFrame
    df_spark = spark.createDataFrame(df_pandas)
    
    # Append chunk to the Spark DataFrame
    data_chunks.append(df_spark)

# Combine all chunks into a single Spark DataFrame
if data_chunks:
    final_df = data_chunks[0]
    for chunk_df in data_chunks[1:]:
        final_df = final_df.union(chunk_df)
    
    # Show the final Spark DataFrame
    final_df.show()

    # Optionally, save the DataFrame to a file or table for further analysis
    final_df.write.format("delta").save("/mnt/app_insights_data")
else:
    print("No data fetched.")

#####################################################################################
#####################################################################################

import requests
import pandas as pd
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("AppInsightsData").getOrCreate()

# Replace with your connection string
connection_string = "<YOUR_CONNECTION_STRING>"

# Extract the endpoint from the connection string
endpoint = connection_string.split("Endpoint=")[1].split(";")[0]
api_url = f"{endpoint}/v1/apps/query"

# Set up headers
headers = {
    "Content-Type": "application/json"
}

def fetch_app_insights_data(query, continuation_token=None):
    body = {
        "query": query
    }
    if continuation_token:
        body["continuationToken"] = continuation_token

    response = requests.post(api_url, headers=headers, json=body)
    response.raise_for_status()  # Raise an exception for any failed requests

    return response.json()

def get_date_ranges(period_start_date, period_end_date, chunk_size_days=30):
    ranges = []
    while period_start_date < period_end_date:
        next_end_date = period_start_date + timedelta(days=chunk_size_days)
        if next_end_date > period_end_date:
            next_end_date = period_end_date
        ranges.append((period_start_date.isoformat(), next_end_date.isoformat()))
        period_start_date = next_end_date
    return ranges

def fetch_data_for_period(query_template, period_start_date, period_end_date, chunk_size_days=30):
    date_ranges = get_date_ranges(period_start_date, period_end_date, chunk_size_days)
    data_chunks = []

    for start_date, end_date in date_ranges:
        query = query_template.format(start_date=start_date, end_date=end_date)
        all_rows = []
        continuation_token = None

        while True:
            data = fetch_app_insights_data(query, continuation_token)

            if "tables" in data and data["tables"]:
                if not all_rows:
                    # Fetch column names from the first response
                    columns = [col["name"] for col in data["tables"][0]["columns"]]
                
                # Collect the data rows
                all_rows.extend(data["tables"][0]["rows"])

            # Check for pagination (continuationToken)
            if "continuationToken" in data:
                continuation_token = data["continuationToken"]
            else:
                break

        # Convert rows to a Pandas DataFrame and append to the Spark DataFrame
        df_pandas = pd.DataFrame(all_rows, columns=columns)
        df_spark = spark.createDataFrame(df_pandas)
        data_chunks.append(df_spark)

    # Combine all chunks into a single Spark DataFrame
    if data_chunks:
        final_df = data_chunks[0]
        for chunk_df in data_chunks[1:]:
            final_df = final_df.union(chunk_df)
        
        # Show the final Spark DataFrame
        final_df.show()

        # Optionally, save the DataFrame to a file or table for further analysis
        final_df.write.format("delta").save("/mnt/app_insights_data")
    else:
        print("No data fetched.")

# Example usage
period_start_date = datetime.now() - timedelta(days=365)  # Configure this for the start of your period
period_end_date = datetime.now()  # Configure this for the end of your period

query_template = """
requests
| where timestamp >= datetime({start_date}) and timestamp < datetime({end_date})
| limit 10000
"""

fetch_data_for_period(query_template, period_start_date, period_end_date, chunk_size_days=30)

#########

import requests
import pandas as pd
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("AppInsightsData").getOrCreate()

# Replace with your connection string
connection_string = "<YOUR_CONNECTION_STRING>"

# Extract the endpoint from the connection string
endpoint = connection_string.split("Endpoint=")[1].split(";")[0]
api_url = f"{endpoint}/v1/apps/query"

# Set up headers
headers = {
    "Content-Type": "application/json"
}

def fetch_app_insights_data(query, continuation_token=None):
    body = {
        "query": query
    }
    if continuation_token:
        body["continuationToken"] = continuation_token

    response = requests.post(api_url, headers=headers, json=body)
    response.raise_for_status()  # Raise an exception for any failed requests

    return response.json()

def get_date_ranges(period_start_date, period_end_date, chunk_size_days=30):
    ranges = []
    while period_start_date < period_end_date:
        next_end_date = period_start_date + timedelta(days=chunk_size_days)
        if next_end_date > period_end_date:
            next_end_date = period_end_date
        ranges.append((period_start_date.isoformat(), next_end_date.isoformat()))
        period_start_date = next_end_date
    return ranges

def fetch_data_for_period(query_template, period_start_date, period_end_date, chunk_size_days=30):
    date_ranges = get_date_ranges(period_start_date, period_end_date, chunk_size_days)
    data_chunks = []

    for start_date, end_date in date_ranges:
        query = query_template.format(start_date=start_date, end_date=end_date)
        all_rows = []
        continuation_token = None

        while True:
            data = fetch_app_insights_data(query, continuation_token)

            if "tables" in data and data["tables"]:
                if not all_rows:
                    # Fetch column names from the first response
                    columns = [col["name"] for col in data["tables"][0]["columns"]]
                
                # Collect the data rows
                all_rows.extend(data["tables"][0]["rows"])

            # Check for pagination (continuationToken)
            if "continuationToken" in data:
                continuation_token = data["continuationToken"]
            else:
                break

        # Convert rows to a Pandas DataFrame and append to the Spark DataFrame
        df_pandas = pd.DataFrame(all_rows, columns=columns)
        df_spark = spark.createDataFrame(df_pandas)
        data_chunks.append(df_spark)

    # Combine all chunks into a single Spark DataFrame
    if data_chunks:
        final_df = data_chunks[0]
        for chunk_df in data_chunks[1:]:
            final_df = final_df.union(chunk_df)
        
        # Show the final Spark DataFrame
        final_df.show()

        # Optionally, save the DataFrame to a file or table for further analysis
        final_df.write.format("delta").save("/mnt/app_insights_data")
    else:
        print("No data fetched.")

# Example usage
period_start_date = datetime.now() - timedelta(days=365)  # Configure this for the start of your period
period_end_date = datetime.now()  # Configure this for the end of your period

query_template = """
union customEvents, traces, dependencies
| where timestamp >= datetime({start_date}) and timestamp < datetime({end_date})
| limit 500000
"""

fetch_data_for_period(query_template, period_start_date, period_end_date, chunk_size_days=30)
