# compare two excel files

import pandas as pd

def compare_excel_sheets(file1, file2):
    # Read both Excel files
    xls1 = pd.ExcelFile(file1)
    xls2 = pd.ExcelFile(file2)

    # Get sheet names from both files
    sheet_names1 = xls1.sheet_names
    sheet_names2 = xls2.sheet_names

    # Check for new sheets
    new_sheets = set(sheet_names2) - set(sheet_names1)
    if new_sheets:
        print("New sheets added in file2:", new_sheets)

    # Iterate over common sheet names
    common_sheets = set(sheet_names1) & set(sheet_names2)
    for sheet_name in common_sheets:
        df1 = pd.read_excel(file1, sheet_name)
        df2 = pd.read_excel(file2, sheet_name)

        # Check if data underneath the sheet changed
        if not df1.equals(df2):
            print(f"Changes detected in sheet '{sheet_name}':")
            # You can add more detailed comparison logic here if needed

# Provide paths to the Excel files
file_path1 = 'file1.xlsx'
file_path2 = 'file2.xlsx'

# Compare Excel files
compare_excel_sheets(file_path1, file_path2)




# download a file from share point to local

from shareplum import Site
from shareplum import Office365

# SharePoint site URL and credentials
site_url = "https://your_sharepoint_site_url"
username = "your_username"
password = "your_password"

# Path to the file on SharePoint
file_path = "/sites/your_site_name/your_document_library/your_file.xlsx"

# Local path where the downloaded file will be saved
local_file_path = "local_path_to_save_downloaded_file/your_file.xlsx"

# Initialize Office365 session
authcookie = Office365(site_url, username=username, password=password).GetCookies()

# Connect to the SharePoint site
site = Site(site_url, authcookie=authcookie)

# Download the file
site.get_file(file_path, local_file_path)


# rename an old excel file in DBFS

# Specify the old and new file paths
old_file_path = "/path/to/old/file.txt"
new_file_path = "/path/to/new/file.txt"

# Rename the file using dbutils.fs.mv() function
dbutils.fs.mv(old_file_path, new_file_path)







import requests
import json

class Utilities:
    __instance = None

    def __init__(self):
        """
        Private constructor to prevent direct instantiation.
        """
        if Utilities.__instance is not None:
            raise Exception("This class is a singleton! Use get_instance() method instead.")
        else:
            Utilities.__instance = self

    @classmethod
    def get_instance(cls):
        """
        Get instance of Utilities class.

        Returns:
            Utilities: Instance of Utilities class.
        """
        if cls.__instance is None:
            cls.__instance = Utilities()
        return cls.__instance

    @staticmethod
    def send_email_via_logic_app(logic_app_url: str, recipient: str, subject: str, body: str):
        """
        Send email via Azure Logic App.

        Args:
            logic_app_url (str): URL of the Azure Logic App endpoint.
            recipient (str): Email address of the recipient.
            subject (str): Subject of the email.
            body (str): Body content of the email.

        Returns:
            None
        """
        headers = {
            'Content-Type': 'application/json'
        }

        email_data = {
            "recipient": recipient,
            "subject": subject,
            "body": body
        }

        try:
            response = requests.post(logic_app_url, headers=headers, data=json.dumps(email_data))
            if response.status_code == 200:
                print("Email sent successfully!")
            else:
                print("Failed to send email. Status code:", response.status_code)
        except Exception as e:
            print("An error occurred while sending email:", e)

    @staticmethod
    def is_prime(number):
        """
        Check if a number is prime.

        Args:
            number (int): Number to check.

        Returns:
            bool: True if the number is prime, False otherwise.
        """
        if number <= 1:
            return False
        for i in range(2, int(number**0.5) + 1):
            if number % i == 0:
                return False
        return True

    def __private_method(self):
        """
        Example private method.
        """
        print("This is a private method.")

    def public_method(self):
        """
        Example public method.
        """
        print("This is a public method.")

# Example usage:
if __name__ == "__main__":
    # Accessing the instance of the Utilities class
    utilities = Utilities.get_instance()

    # Using the send_email_via_logic_app static method
    logic_app_url = "YOUR_LOGIC_APP_URL_HERE"
    recipient = "recipient@example.com"
    subject = "Test Email"
    body = "This is a test email sent via Azure Logic App."
    utilities.send_email_via_logic_app(logic_app_url, recipient, subject, body)

    # Using public method
    utilities.public_method()

    # Accessing a static method
    print("Is 7 a prime number?", Utilities.is_prime(7))

    # Attempting to access private method (will raise AttributeError)
    try:
        utilities.__private_method()
    except AttributeError:
        print("Cannot access private method directly.")



import logging
from opencensus.ext.azure.log_exporter import AzureLogHandler

def configure_logger():
    logger = logging.getLogger(__name__)
    
    # Remove all existing handlers
    while logger.handlers:
        logger.removeHandler(logger.handlers[0])

    # Add Azure Log Handler only if it does not exist
    if not any(isinstance(handler, AzureLogHandler) for handler in logger.handlers):
        handler = AzureLogHandler(connection_string='InstrumentationKey=your_instrumentation_key')
        logger.addHandler(handler)
    
    # Set logging level
    logger.setLevel(logging.INFO)

    return logger

# Configure logger
logger = configure_logger()

# Log a test message
logger.info("This is a test log message.")







1. Coding Questions: Pyspark:
You've been given some CSV files like karnataka.csv and maharashtra.csv in an ADLS location, each containing columns for first_name, last_name, age, sex, and location.
Your task is to add a new column called state to each DataFrame. The state column should contain the state name extracted from the filename.
For example:
For karnataka.csv, the state column should contain the value 'karnataka'.
For maharashtra.csv, the state column should contain the value 'maharashtra'.
Your solution should utilize PySpark to efficiently handle large-scale data processing tasks.

2. Coding Question: Python/ DSA:
A. Reverse a string without using reverse function
B. Write a python program to implement following. Given a pattern and a string. If string matches the pattern return true else return false. 
Ex: 
Pattern – “abba”, String – “dog cat cat dog” will return true. 
Pattern – “aba”, String – “dog dog cat” will return false.

3. Coding Question: SQL
Find the 3-month rolling average of total revenue from purchases given a table with users, their purchase amount, and date purchased (YYYY-MM-DD). Output the year-month (YYYY-MM) and 3-month rolling average of revenue, sorted from earliest month to latest month.

4. Explain Spark Architecture.

5. What is Z-ordering?

6. Difference between Datamart, Datawarehouse and Deltalake

7. Compare performance of Managed and External Table




- Python + Pandas = Data Manipulation
- Python + Pyspark = Big Data Processing
- Python + Scikit-Learn = Machine Learning
- Python + TensorFlow = Deep Learning
- Python + Matplotlib = Data Visualization
- Python + Seaborn = Advanced Visualization
- Python + Flask = Web Development
- Python + Pygame = Game Development
- Python + Kivy = Mobile App Development
- Python + Django = Full Stack Web Development
- Python + NumPy = Numerical Computation
- Python + Keras = Neural Networks
- Python + Plotly = Interactive Visualization
- Python + FastAPI = Fast Web APIs
- Python + Tkinter = GUI Development
- Python + BeautifulSoup = Web Scraping
- Python + Scrapy = Web Crawling
- Python + OpenCV = Computer Vision
- Python + Pillow = Image Processing
- Python + SQLAlchemy = Database Management
- Python + Requests = HTTP Requests
- Python + PyTorch = Deep Learning
- Python + NLTK = Natural Language Processing
- Python + spaCy = Natural Language Processing
- Python + Bokeh = Interactive Visualization
- Python + Dash = Web-based Data Visualization
- Python + Pydantic = Data Validation
- Python + Celery = Task Queue Management
- Python + Twisted = Network Programming
- Python + Asyncio = Asynchronous Programming


##############################App Insight#################################################################


To achieve your requirements, you can follow these steps:

Send logs from Databricks to Azure Application Insights.
Retrieve logs from Application Insights.
Send the logs via email to users.
Here's an example Python code that covers these steps:

Step 1: Send logs from Databricks to Azure Application Insights
First, you need to install the applicationinsights library on your Databricks cluster:

sh
Copy code
pip install applicationinsights
Now, create a function in Databricks to send logs to Azure Application Insights:

python
Copy code
from applicationinsights import TelemetryClient

def send_log_to_appinsights(instrumentation_key, message, properties=None):
    tc = TelemetryClient(instrumentation_key)
    tc.track_event(message, properties)
    tc.flush()

# Usage example:
instrumentation_key = "YOUR_APPINSIGHTS_INSTRUMENTATION_KEY"
send_log_to_appinsights(instrumentation_key, "This is a test log", {"property1": "value1", "property2": "value2"})
Step 2: Retrieve logs from Application Insights
To retrieve logs from Application Insights, you need to query the logs using the Application Insights REST API or Azure Monitor Query SDK. Here, we'll use the REST API for simplicity:

First, install the required library:

sh
Copy code
pip install azure-identity azure-monitor-query
Now, create a function to retrieve logs:

python
Copy code
import requests
from azure.identity import DefaultAzureCredential
from azure.monitor.query import LogsQueryClient

def retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query):
    credential = DefaultAzureCredential()
    client = LogsQueryClient(credential)

    response = client.query_workspace(
        workspace_id=app_id,
        query=query
    )

    return response.tables

# Usage example:
app_id = "YOUR_APPINSIGHTS_APP_ID"
tenant_id = "YOUR_TENANT_ID"
client_id = "YOUR_CLIENT_ID"
client_secret = "YOUR_CLIENT_SECRET"
query = "requests | take 10"

logs = retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query)
Step 3: Send the logs via email to users
Install the smtplib and email libraries:

sh
Copy code
pip install secure-smtplib
Now, create a function to send the logs via email:

python
Copy code
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

def send_email(smtp_server, smtp_port, smtp_user, smtp_password, from_email, to_emails, subject, body):
    msg = MIMEMultipart()
    msg['From'] = from_email
    msg['To'] = ", ".join(to_emails)
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))

    server = smtplib.SMTP(smtp_server, smtp_port)
    server.starttls()
    server.login(smtp_user, smtp_password)
    server.sendmail(from_email, to_emails, msg.as_string())
    server.quit()

# Usage example:
smtp_server = "smtp.example.com"
smtp_port = 587
smtp_user = "your_smtp_username"
smtp_password = "your_smtp_password"
from_email = "from@example.com"
to_emails = ["to1@example.com", "to2@example.com"]
subject = "App Insights Logs"
body = "Here are the logs: \n" + str(logs)

send_email(smtp_server, smtp_port, smtp_user, smtp_password, from_email, to_emails, subject, body)
Putting it all together
You can create a function that combines all these steps:

python
Copy code
def main():
    # Step 1: Send logs to Application Insights
    instrumentation_key = "YOUR_APPINSIGHTS_INSTRUMENTATION_KEY"
    send_log_to_appinsights(instrumentation_key, "This is a test log", {"property1": "value1", "property2": "value2"})
    
    # Step 2: Retrieve logs from Application Insights
    app_id = "YOUR_APPINSIGHTS_APP_ID"
    tenant_id = "YOUR_TENANT_ID"
    client_id = "YOUR_CLIENT_ID"
    client_secret = "YOUR_CLIENT_SECRET"
    query = "requests | take 10"
    
    logs = retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query)
    
    # Step 3: Send the logs via email to users
    smtp_server = "smtp.example.com"
    smtp_port = 587
    smtp_user = "your_smtp_username"
    smtp_password = "your_smtp_password"
    from_email = "from@example.com"
    to_emails = ["to1@example.com", "to2@example.com"]
    subject = "App Insights Logs"
    body = "Here are the logs: \n" + str(logs)
    
    send_email(smtp_server, smtp_port, smtp_user, smtp_password, from_email, to_emails, subject, body)

if __name__ == "__main__":
    main()
Make sure to replace placeholders with your actual values for instrumentation_key, app_id, tenant_id, client_id, client_secret, smtp_server, smtp_port, smtp_user, smtp_password, from_email, and to_emails.

This script will:

Send a log to Application Insights.
Retrieve the latest logs from Application Insights.
Send the retrieved logs to the specified email addresses.
I don't have smtp username password, suggest using appinsight alerts
ChatGPT
If you don't have SMTP credentials, you can use Azure Application Insights alerts to send logs or notifications via email. Azure Application Insights provides built-in alerting capabilities that allow you to trigger actions based on log data and metrics. You can configure alerts to send emails directly from the Azure portal without needing an SMTP server.

Here’s how to set up and configure Application Insights alerts to send logs via email:

Step 1: Create a Log Alert in Azure Application Insights
Navigate to Application Insights: Go to the Azure portal and navigate to your Application Insights resource.

Create a Log Alert Rule:

Select "Alerts" from the left-hand menu.
Click on "New alert rule."
Define the scope: Select the Application Insights resource you want to monitor.
Add a condition: Click "Add condition," and select "Custom log search."
Define your query: Enter a Kusto Query Language (KQL) query to define the logs you want to alert on. For example, to alert on failed requests:
kql
Copy code
requests
| where resultCode != "200"
Set the alert logic: Configure the condition to trigger when the number of results meets your threshold.
Configure the Action Group:

Click "Add action group."
Name the action group and provide a short name.
Select "Action type" as "Email/SMS/Push/Voice."
Configure email settings: Enter the email addresses you want to receive the alerts.
Save the action group.
Create and Enable the Alert Rule:

Give the alert rule a name and description.
Set the severity level.
Review and create the alert rule.
Step 2: Retrieve Logs from Application Insights via API (Optional)
If you need to retrieve logs programmatically and send them via another method, you can use the Application Insights REST API or Azure Monitor Query SDK. Here's an example using the REST API:

python
Copy code
import requests
from azure.identity import DefaultAzureCredential
from azure.monitor.query import LogsQueryClient

def retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query):
    credential = DefaultAzureCredential()
    client = LogsQueryClient(credential)

    response = client.query_workspace(
        workspace_id=app_id,
        query=query
    )

    return response.tables

# Usage example:
app_id = "YOUR_APPINSIGHTS_APP_ID"
tenant_id = "YOUR_TENANT_ID"
client_id = "YOUR_CLIENT_ID"
client_secret = "YOUR_CLIENT_SECRET"
query = "requests | where resultCode != '200' | take 10"

logs = retrieve_logs_from_appinsights(app_id, tenant_id, client_id, client_secret, query)
print(logs)
Step 3: Using Logic Apps for Additional Email Customization (Optional)
If you need more customization for your email notifications, you can use Azure Logic Apps to send emails based on Application Insights alerts.

Create a Logic App: In the Azure portal, create a new Logic App.
Define the Trigger: Use "When a HTTP request is received" as the trigger.
Add Actions:
Parse the JSON received from the alert.
Use the "Send an email" action to configure and send the email.
Integrate with Application Insights: In the alert rule configuration, use the Logic App as the action group.
By using Azure Application Insights alerts and potentially integrating with Logic Apps, you can effectively monitor and send logs via email without needing SMTP credentials.


To retrieve Application Insights logs using Python, you can use the azure-monitor-query package. This package allows you to query Application Insights logs with a specific instrumentation key. Below is a step-by-step guide on how to set this up:

Step 1: Install the necessary package
First, install the azure-monitor-query package if you haven't already:

bash
Copy code
pip install azure-monitor-query
Step 2: Set up the Azure credentials
You'll need Azure credentials to authenticate your queries. You can use Azure Service Principal, Azure Managed Identity, or Azure CLI login.

Here's an example using the DefaultAzureCredential:

python
Copy code
from azure.identity import DefaultAzureCredential
from azure.monitor.query import LogsQueryClient

# Create a LogsQueryClient with DefaultAzureCredential
credential = DefaultAzureCredential()
client = LogsQueryClient(credential)
Step 3: Define your query and execute it
You can define your Kusto Query Language (KQL) query and execute it to retrieve the logs. Here’s an example:

python
Copy code
from datetime import datetime, timedelta

# Define the Application Insights query
query = """
traces
| where timestamp > ago(1h)
| project timestamp, message, severityLevel
| order by timestamp desc
"""

# Replace 'your-instrumentation-key' with your actual instrumentation key
application_id = 'your-instrumentation-key'

# Execute the query
response = client.query_workspace(
    workspace_id=application_id,
    query=query,
    timespan=(datetime.now() - timedelta(hours=1), datetime.now())
)

# Print the results
if response.status == 'Success':
    for table in response.tables:
        for row in table.rows:
            print(row)
else:
    print(f"Query failed with status: {response.status}")
Step 4: Handling authentication
Make sure to handle authentication properly. If you’re using Azure Service Principal, you can use the following code to authenticate:

python
Copy code
from azure.identity import ClientSecretCredential

tenant_id = 'your-tenant-id'
client_id = 'your-client-id'
client_secret = 'your-client-secret'

credential = ClientSecretCredential(tenant_id, client_id, client_secret)
client = LogsQueryClient(credential)
Full Example
python
Copy code
from azure.identity import DefaultAzureCredential
from azure.monitor.query import LogsQueryClient
from datetime import datetime, timedelta

# Set up the credential and client
credential = DefaultAzureCredential()
client = LogsQueryClient(credential)

# Define the Application Insights query
query = """
traces
| where timestamp > ago(1h)
| project timestamp, message, severityLevel
| order by timestamp desc
"""

# Replace 'your-instrumentation-key' with your actual instrumentation key
application_id = 'your-instrumentation-key'

# Execute the query
response = client.query_workspace(
    workspace_id=application_id,
    query=query,
    timespan=(datetime.now() - timedelta(hours=1), datetime.now())
)

# Print the results
if response.status == 'Success':
    for table in response.tables:
        for row in table.rows:
            print(row)
else:
    print(f"Query failed with status: {response.status}")
Replace 'your-instrumentation-key', 'your-tenant-id', 'your-client-id', and 'your-client-secret' with your actual values. This script will retrieve the logs from the last hour and print them.

Make sure to manage your credentials securely and avoid hardcoding them in your script. Use environment variables or Azure Key Vault for sensitive information.







from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkSession
spark = SparkSession.builder.appName("SampleDataFrame").getOrCreate()

# Sample DataFrame creation for demonstration
data = [
    ("California", 1), ("California", 2), ("California", 3),
    ("New York", 4), ("New York", 5), ("New York", 6),
    ("Texas", 7), ("Texas", 8), ("Texas", 9),
    # Add more records as needed
]
columns = ["state", "value"]
df = spark.createDataFrame(data, columns)

# Display the original DataFrame
df.show()

# Take 10% of records per state
fractions = df.select("state").distinct().withColumn("fraction", lit(0.1)).rdd.collectAsMap()
sampled_df = df.sampleBy("state", fractions, seed=42)

# Display the sampled DataFrame
sampled_df.show()

# Stop the SparkSession
spark.stop()



## write output to a file in databricks


import json
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("AppendJSON").getOrCreate()

def append_json_to_file(json_content, file_path="/dbfs/tmp/json_data.txt"):
    """
    Appends JSON content to a file in DBFS.
    
    :param json_content: The JSON content to append (dictionary).
    :param file_path: The path to the file in DBFS.
    """
    try:
        # Read existing content if file exists
        try:
            with open(file_path, "r") as file:
                data = file.read()
                if data:
                    existing_data = json.loads(data)
                else:
                    existing_data = []
        except FileNotFoundError:
            existing_data = []
        
        # Append new JSON content
        existing_data.append(json_content)
        
        # Write updated content back to file
        with open(file_path, "w") as file:
            file.write(json.dumps(existing_data, indent=4))
        
        print("JSON content appended successfully.")
    
    except Exception as e:
        print(f"An error occurred: {e}")

# Example JSON content
json_content_example = {
    "name": "example",
    "value": 42
}

# Append example JSON content to file
append_json_to_file(json_content_example)



import json
import os
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("AppendJSON").getOrCreate()

def create_or_clear_file(file_path="/dbfs/tmp/json_data.txt"):
    """
    Creates a file or clears its content if it already exists.
    
    :param file_path: The path to the file in DBFS.
    """
    with open(file_path, "w") as file:
        file.write("")

def append_json_to_file(json_content, file_path="/dbfs/tmp/json_data.txt"):
    """
    Appends JSON content to a file in DBFS.
    
    :param json_content: The JSON content to append (dictionary).
    :param file_path: The path to the file in DBFS.
    """
    try:
        # Read existing content if file exists and is not empty
        try:
            with open(file_path, "r") as file:
                data = file.read()
                if data.strip():  # Check if file is not empty
                    existing_data = json.loads(data)
                else:
                    existing_data = []
        except FileNotFoundError:
            existing_data = []
        
        # Append new JSON content
        existing_data.append(json_content)
        
        # Write updated content back to file
        with open(file_path, "w") as file:
            file.write(json.dumps(existing_data, indent=4))
        
        print("JSON content appended successfully.")
    
    except Exception as e:
        print(f"An error occurred: {e}")

# Create or clear the file first
create_or_clear_file()

# Example JSON content
json_content_example = {
    "name": "example",
    "value": 42
}

# Append example JSON content to file
append_json_to_file(json_content_example)




Build a databricks widget to compare 2 files.

a> It should take the file input type as excel, csv and JSON and compare the files, after converting both files csv.
b> It should then print rows in first file , not in second file , using exceptAll
c> Then print rows in second file not in first file, using exceptAll
d> Now within the rertreived differences in point b and c , find what column values were actually different and what values were same, using address and building_id as the key


# Import necessary libraries
import pandas as pd
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("FileComparison").getOrCreate()

# Create widgets for file input
dbutils.widgets.text("file_path_1", "")
dbutils.widgets.text("file_path_2", "")
dbutils.widgets.dropdown("file_type_1", "csv", ["csv", "json", "excel"])
dbutils.widgets.dropdown("file_type_2", "csv", ["csv", "json", "excel"])

# Read widget values
file_path_1 = dbutils.widgets.get("file_path_1")
file_path_2 = dbutils.widgets.get("file_path_2")
file_type_1 = dbutils.widgets.get("file_type_1")
file_type_2 = dbutils.widgets.get("file_type_2")

# Function to read different file types and convert to Spark DataFrame
def read_file(file_path, file_type):
    if file_type == "csv":
        return spark.read.option("header", "true").csv(file_path)
    elif file_type == "json":
        return spark.read.json(file_path)
    elif file_type == "excel":
        pdf = pd.read_excel(file_path)
        return spark.createDataFrame(pdf)

# Read both files
df1 = read_file(file_path_1, file_type_1)
df2 = read_file(file_path_2, file_type_2)

# Convert DataFrames to have the same schema by selecting common columns
common_cols = list(set(df1.columns).intersection(set(df2.columns)))
df1 = df1.select(common_cols)
df2 = df2.select(common_cols)

# Print rows in first file not in second file
print("Rows in first file not in second file:")
df1_diff = df1.exceptAll(df2)
df1_diff.show()

# Print rows in second file not in first file
print("Rows in second file not in first file:")
df2_diff = df2.exceptAll(df1)
df2_diff.show()

# Find column differences within the retrieved differences
key_columns = ["address", "building_id"]

# Create a function to find differences in columns
def find_differences(df1, df2, key_cols):
    df1_keyed = df1.select(key_cols + [col for col in df1.columns if col not in key_cols])
    df2_keyed = df2.select(key_cols + [col for col in df2.columns if col not in key_cols])
    df1_keyed = df1_keyed.withColumnRenamed("address", "address_1").withColumnRenamed("building_id", "building_id_1")
    df2_keyed = df2_keyed.withColumnRenamed("address", "address_2").withColumnRenamed("building_id", "building_id_2")
    diff_df = df1_keyed.join(df2_keyed, (df1_keyed.address_1 == df2_keyed.address_2) & (df1_keyed.building_id_1 == df2_keyed.building_id_2), "outer")
    diff_df = diff_df.withColumn("Same/Changed", col("address_1") == col("address_2") & col("building_id_1") == col("building_id_2"))
    return diff_df

# Apply the function to the differences
diff_df = find_differences(df1_diff, df2_diff, key_columns)
diff_df.show()

